Progress:
1) DONE write tts class and basic methods and unit tests of these
2) DONE write wt class and basic methods and unit tests of these
3) DONE write wmf class and basic methods and unit tests of these 
4) DONE write wpmf class and basic methods and unit tests of these 
4.1) DONE make your error handling functions screen for non-finite values and throw an error
4.2) DONE make the wmpf class have a significance slot, and make accompanying methods deal with it, and add to the unit tests for those methods if necessary, and make the generato function compute significance in a few diff ways (fft, aaft, standard quick way).
  -DONE get surrogate function(s), unit tests for surrogate function 
  -DONE change the generator function
  -DONE change the methods to reflect/impose the new class definition for wpmf
  -DONE unit tests for new aspects of generator
  -DONE unit tests for new aspects of methods
  -DONE go back and use log 2 on the timescales axis in plots in tests_wmf
5) DONE check and unit test cleandat
6) DONE write the magnitude plotter functions, and unit tests. perhaps these should go in the methods files? maybe not. think more
6.5) DONE Go back and add joint Box-Cox to cleandat, and unit tests
6.75) DONE get the .Rmds going and compile everything as a package
8) DONE start a vignette, covering the code you have written so far - done except for surrogates, have to do that section, but leaving that until after coherence stuff is written
9) DONE compile again as a package
10) DONE stopping point - mean field methods implemented. next step coherence methods.
11) DONE Work on coherence functionality, inc unit tests and vignette and making the package compile. Need to start with a design phase and working out appropriate S3 classes. See some thoughts below. - IMPLEMENTED COH, STILL NEED PLOTTERS, THENA  VIGNETTE
DONE addranks
DONE cohbandsignif
DONE plotmag
DONE plotrank
DONE plotphase
DONE avgphase
DONE vignette - make sure to cover all these new functions
12) DONE design phase for wavelet linear models
13) to do for wavelet linear models:
wlm and tests - done except need one more test for a realistic case, but I wrote a plan for that case in the testing file 
DONE wlm set and get methods 
wlmtest generator function and accompanying wlmfit internal, and tests - wrote the functions but have not tested yet, also have not yet implemented the fast algorithm
DONE do you want to add slots for scale.min, scale.max.input, sigma, and f0 to wt, wmf, wpmf, and coh methods? You would also need set and get methods for these slots for each class. Or maybe stick them together into a list called wopts which makes a single slot, and then you would have to make a similar change to wlm.
wlmtest methods, and tests thereof
plotmag method for wlmtest and tests - docs to be combined with plotmag.coh
plotrank method for wlmtest and tests - docs to be combined with plotrank.coh
tests for addranks for wlmtest objects
bandtest for wlmtest objects, and tests - docs combned with coh method
predsync and tests
power function for tts class and tests
modsync and tests
propexpl and tests - proportion of synchrony explained as a function of timescale and related quantities
looscore and tests
locassoc tester function and tests
examining if phases of coefficients are consistent with predictor/response phases - do this later, together with functionality for plotting phases of various things (I have left a lot of phase plot functions out)

12) everything you need for deer
13) synmat for kelp and Jon's bird stuff
14) write the phase plotter functions, and unit tests, for tts, wt, wpmd, wmf. 
15) print and summary methods for all the classes


for formatting the documentation at top:
1) use \code{} for TRUE and FALSE. Do not use T and F.
2) I foudn and deleted a comment in the header of wmpf to the effect that "the wavelet phasor mean field was developed by Sheppard and Reuman, R code written by Anderson and Walter." It was not developed by Shppard and Reuman, and now the R code is by all of us, so I changed it. Watch out for other notes like that, I think I remember seeing some more.

general
1) DECIDED NOT TO BOTHER It will be possible to be a bit more consistent with use of get and set methods instead of $
2) DONE you need an error checking function for scale.min, scale.max, f0, sigma, and you need to call it from the top of all functions that take these as arguments. Also
indicate in vignette and in help files what the constraints are. 
3) DONE times can ONLY be spaced with units of one, otherwise the code does not
work as it is. This is a fundamental problem and would require changes in lots of 
places if we wanted to add the flexibility of times spaced other than 1. So just add this requirement to your error checking. Also in docs and vignette. Everything is in 
cycles per sample - Lawrence assumes this in his matlab code. 
4) do a test case with Lawrence for non-integer scale.min. For that matter do some
testing with non dedault values of all of scale.min, scale.max, f0, sigma. One thing to test is, if you use scale.max.input very large, then scallopping should produce
the actual largest timescale and this should be the same for even larger scale.max.input. I said this was the case in the vignette (toward the end of the wt section), so if it turns out not to be the case, go back and change that part of the vignette
5) DONE Add a paragraph to the vignette explaining what scale.min, scale.max, f0, sigma,
or ask Lawrence to do it. 
6) DONE test coh with vectors, also with 1 by N matrices
7) right now, when using the fast method, coh does not work with norm=powind
8) DONE error check the fast algorithm, n=1 case
9) Add examples to example sections of docs everywhere

wt.r:
#TODO: make this a compiled function using cmpfun in the compiler package
#(which is part of base R)
#TODO: make the truncation occur before the computation rather than after
#TODO: Not clear from the docs what the resulting behavior is for the default
#value NULL of scale.max.input. Likewise for other related funcs (wmf, wpmf, etc)

________

Design phase for wavelet linear models:

Functionality needed:
*Fit a given model - pass in data, and a normalization to use (or should we only use the power normalization), and it does the fitting and gets coefs that depend on timescale (only, not on loction). It will also need scale.min, f0, etc.
*May want the capacity to plot the coefficients in various ways, e.g., plot their phases or magnitudes against timescale, but if you do this, do it much later
*Compare two models. Will need to be able to plot coherences and ranks, and get p-values over a timescale band. Will have to work for fft or aaft surrogates, or via the fast algorithm. May or may not want to have the option of joint surrogates of multiple variables being dropped versus independent (across the variables) surrogates. Not sure if the latter makes any sense - discuss with Lawrence. Of course, spatially, we always use joint surrogates.
*model-predicted synchrony plot
*think a bit to make sure the design would permit reasonable addition, later, of functionality for doing the leave-one-out cross validation and model selection, but don't do it now.
*proportion of synchrony explained by a model, as a function of timescale (and this can be averaged over a timescale band). See eq 47 in the supp mat of the plankton paper.
*cross terms as a proportion of synchrony, as a function of timescale (and this can be averaged over a band). See eq. 48 of supp mat of plankton paper
*do you want to be able to plot model synchrony or mean squared model synchrony? (remember this is distinct from model-predicted synchrony.) We used this for thinking of interaction effects.
*do anything else with interaction effects, e.g., test their significance?
*fractions of synchrony explained by individual predictors, and by interaction effects, as a function of timescale. Also can average these over bands.

Thoughts:
have a wlm (wavelet linear model) class with these slots:
dat: a list with the data matrices, probably the first is the response
times: the times of measurement
wts: a list with all the wavelet transforms (do you actually want this? I don't see why not except if we end up creating one of these objects for each of many surrogates, we won't want to keep all the transforms). Perhaps it should be powall normalized transforms?
timescales: 
coefs: a list with coefficients - these are complex-valued functions of timescale, one for each predictor
d2/resids: the measurement of how good the model is (also a function of timescale, I think)
coher: coherence of model with response (do you actually want this? or perhaps we want it even more than d2/resids?)

You need a fast, internal function that fits pre-computed transforms and just returns the coefficients and the residuals. Call this (for now) wlm_fit_intern. Or maybe it also returns coherence of the fitted model with the response. Or maybe only.

You need a fitter function (user side) that takes data (and other arguments) and produces a wlm object - this is the creator for a wlm object

You need a wlm_test (temporary name) function that takes a wlm object and other arguments and performs significance tests for one or more variables as specified in the other arguments. This will have several arguments specifying the nature of the test (fft surrog, aaft surrog, fast, possibly also joint v independent surrogates of the separate surrogated variables). For fft surrog or aaft surrog, this generates surrogates of the variables to be tested, recomputes transforms, then calls wlm_fit_intern, then keeps all the residual vectors (and maybe also or instead coherences of the models with the data) and throws away the coefficients, then returns this stuff somehow. 
  Returns another type of object, or embedded in the wlm object? If the latter, then it will be easier to loose track of which test objects go with which model object, but if the latter the model object can become very complex. perhaps we want a wlm_test class whose first slot is a wlm object, indicating the model being tested. Next slots can store the variables that are removed for the test, and the algorithm used (fft, aaft, fast). Next slots after that can store the residuals for each surrogating, and in addition or instead model/response coherences for each surrogating.
  For the fast algorithm things will prbably be different, and will probably start from the data in the wlm object instead of from the transforms

Lawrence tests based on residuals d, but I would prefer to plot and test based on coherences. Mathematically it is equivalent but I would prefer to see plots that show coherence versus timescale for the real model and then for versions where some predictors have been replaced with surrogates. The problem with this is, defining coherence requires that one choose a normalization, and we only know that d minimization and coherence maximization are related for powall normalization. So at a minimum one needs to think carefully and do some math about how normalization and fitting and coherence interact. But I think actually powall is baked into the pie for this stuff, and I just will be committing to it, but check this with Lawrence.

I have some code here from before that is probably pretty good and useful: Reuman/ResearchStaff/PeopleInAction/LawrenceEmmaSVN_2015/CodePackages/WaveletCode/Waveletanalysis/

Everything has to work equally for single or multiple time series per variable.

What about normalization options? When do those come in? I think they probably are not involved with the actual fitting (NOT TRUE THEY ARE), they only come in at that stage of working out fractions of synchrony explained and so on? Or is the powall normalization baked into the pie? It certainly is baked in to the math in Appendix S7 in the plankton paper.

Qs for Lawrence
1) joint vs. independent surrogates of separate predictor variables when testing multiple predictor variables? Or does only joint make sense? Does the fast algorithm support simultaneous removal of more than one variable? I assume in that case it'll be joint surrogates that are implemented? Could independent surrogs be implemented, if they make any sense?
Lawrence and Dan decided to only implement the joint surrogates  - cannot find a case where it would make sense to do the indepednent surrgates
2) Inputs and outputs of fast algorithm? Does it start from the data, or the transforms? What does it return?
Args: data matrices
Outputs: coherence between model and response variable, and/or squared norm of d, the residuals. Actually, these things do not count as outputs because Lawrence does not normalize in the same way, he uses these items (which are not the same as what he would get if he normalized) to get ranks (which are the same as what he would get). So actually the rank information is what is outputted, specifically modelpow and surrmodelpow, where: modelpow is power of response minus norm of d^2. Lawrence reckons he could fairly trivially post-hoc implement the right normalization. Lawrence says I can assume his algorithm will take daata and specs on which vars are dropped and spit out coher (powall norm) for model/data for real and surrogates.
3) Is powall normalization baked into the pie? 
See above - make it so you can put in powall, powind, or none, and make it throw a not yet implemented error for powind - Lawrence thinks he can do this later. Currently it does none, it can do powall with a rescaling, and powind will take work. For the slow algorithm, there may be some thought required for both none and definitely for powind, so maybe just implement powall right now and throw and error for the others.



_____

Next step in design:

DONE wlm class:
slots:
dat - a list of matrices, all the same dimensions, representing the data, the first is the response
times - times of measurement
norm - the normalization used 
wts - list of transforms, normalized as specified in argument to generator function, but only powall works now, other options throw an error
timescales - 
coefs - list of complex vectors, each of length the same as timescales. these are the model coefs
coher - appropriately normalized version of coherence of the  model and the response transforms, as specified in norm argument to geneator function, but only powall works for now

DONE wlm generator function:
Args:
dat - a list of matrices representing the data (or in the case of 1 location, a list of vectors)
times - times of measurement
resp - an index in dat of the response matrix
pred - a vector of indices in dat of predictor matrices
norm - powall, none, powind (phase not allowed). Everything but powall throws and error for now
Returns:
A wlm object with the fitted model
What it does:
-rearrange dat to put the response first and keep only the used predictor matrices after that; use vectors when the input data were vectors 
-error check for things like all data matrices/vectors have same dimensions
-compute all transforms to get wts, which should always be locations by times by timescales, even when there is only one location
-pass wts to a fast internal fitting function to get the other slot values
-put it all together into a list and set the class attribute

DONE wlm_fit (an internal function, no error checking):
Args: 
wts - the list of normalized transforms, response variable first
norm - possibly need the normalization used, either powall, none, or powind; everything but powall throws and error for now
Resturns: A list with these elements:
coefs - list of complex vectors, each of length the same as timescales. these are the model coefs
coher - coherence of the model and response transforms, normalization appropriate for the norm argument (function of timescale, real valued)

wlmtest class:
slots:
wlm - a wlm object, the model being tested
drop - either names or indices of variables in wlm$dat that are being dropped. Some error checking is needed to make sure these are legit, e.g., not the response variable
signif - list with coher (appropriately normalized coherence) and scoher (appropriate normalized coherence for surrogates), signif$coher the same as wlm$coher, except for "fast" where these can differ
ranks - similar to the ranks slot for the coh class. As in that case, not filled in on the first call to wlmtest. I rewrote addranks so it will work on either a coh or a wlmtest object. 
bandp - similar to coh class. cohbandtest (now bandtest) is already a method for the coh class, so I will just need to add a method for the wlmtest class. To avoid repeating myself I may want to take guts of the routine and put it into a worker function called by both methods. 

wlmtest generator function:
Args:
wlm -  a wlm object
drop - indictaes which variables are dropped, see arg with same name for wlmtest class
sigmethod - either fft, aaft, fast
Returns: 
A wlmtest object
Notes:
The only combinations of wlm$norm and sigmethod that will work at first will be powall with anything

plotmag and plotrank methods for wlmtest objects should be pretty straightforward, as well as you will need something like addranks and cohbandtest - see thoughts above in ranks and bandp slots of the wlmtest object, some of this is already done (addranks), and some of it has been queued up (bandtest)

You may want to have a plotcoef method for the wlm class, but this should be thought about together with plotting the phase and maybe magnitude of the response times the conjugate of the predictor, as we did in the plankton paper to help validate the models there - do this later

_____


Think about whether this design will work with the following outstanding issues:

*model-predicted synchrony plot

>>>make a function predsync that takes a wlm objetc and returns a tts object which is what we give on line 413 of the supp mat of the plankton paper, but without the magnitudes (and recall the Pi there is already a nonnegative real number for each timescale, before taking magnitudes). Then that can be plotted easily with the plotmag method of the tts class, or its plot v timescale of the mean over time of the squared magnitude. 
I could write a method for a tts object that plots, vs timescale, the mean, over time, of the square magnitude. That would be an easy function. Should I call it power? Remind yourself what power is officially defined as, but regardless this would be easy. Yes, I checked with Lawrence, this is power.
Then I could easily plot model-predicted synchrony

*do you want to be able to plot model synchrony or mean squared model synchrony? (remember this is distinct from model-predicted synchrony.) We used this for thinking of interaction effects.

>>>make a function modsync that takes a wlm object and returns a tts object which is what we denote r_\sigma^{(h)}(t) in the plankton paper. This is a complex-valued function of time and timescale. We can then plot its magnitude using the plotmag method of the tts class. If you want to average the square of this magnitude over time, and then plot that (which is what we do in the plankton paper), it will be easy to do so with your own code. Or I could write a method for a tts object that plots, vs timescale, the mean, over time, of the square magnitude. That would be an easy function. Should I call it power? Remind yourself what power is officially defined as, but regardless this would be easy. Yes, I checked with Lawrence, this is power.

*think a bit to make sure the design would permit reasonable addition, later, of functionality for doing the leave-one-out cross validation and model selection, but don't do it now. 

>>>I think this should be possible after the fact. You'll just write a function that computes a leave-one-out score, and that will make it possible for the user to get the score and compare for different models. The biggest thing to figure out is, the norm squared residuals are used in the description on p.8 of the sup mat of the plankton paper, but I was hoping to not have to ever compute residuals, just use the coherence of model with data. But I think that can be resolved later.  

*Maybe also think about the "local ascociations" test you did. 

>>>Probably the local associations stuff can be left out - would not be too hard with existing code. But if you do later want a local associations test, it would be easy to write a separate function that does that. It would just take two data matrices, just like coh does. Just re-read the section on location permutations in the plankton paper supp mat and you will see this would be a straightforward later addition. In fact I decided this should be done at some point, it won't be hard.

*Some other blocks of text I have elsewhere have started thinking about testing phases of coefficients

>>>Can safely leave this for later, I think

*proportion of synchrony explained by a model, as a function of timescale (and this can be averaged over a timescale band). See eq 47 in the supp mat of the plankton paper.
*cross terms as a proportion of synchrony, as a function of timescale (and this can be averaged over a band). See eq. 48 of supp mat of plankton paper
*fractions of synchrony explained by individual predictors, and by interaction effects, as a function of timescale. Also can average these over bands.

>>>Make one function that takes a wlm object and gives a matrix with named rows:
timescales
propexplained
crossterms
pred1
...
predn
interactions
int1w2
int1w3
...
intnwn-1
Then these can easily be plotted versus timescale or averaged over any band.

*do anything else with interaction effects, e.g., test their significance?

>>>Spoke with Lawrence and we think this could be handled pretty easy with the high-level functions already planned above, and no additional function is needed.

