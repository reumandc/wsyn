Progress:
1) DONE write tts class and basic methods and unit tests of these
2) DONE write wt class and basic methods and unit tests of these
3) DONE write wmf class and basic methods and unit tests of these 
4) DONE write wpmf class and basic methods and unit tests of these 
4.1) DONE make your error handling functions screen for non-finite values and throw an error
4.2) DONE make the wmpf class have a significance slot, and make accompanying methods deal with it, and add to the unit tests for those methods if necessary, and make the generato function compute significance in a few diff ways (fft, aaft, standard quick way).
  -DONE get surrogate function(s), unit tests for surrogate function 
  -DONE change the generator function
  -DONE change the methods to reflect/impose the new class definition for wpmf
  -DONE unit tests for new aspects of generator
  -DONE unit tests for new aspects of methods
  -DONE go back and use log 2 on the timescales axis in plots in tests_wmf
5) DONE check and unit test cleandat
6) DONE write the magnitude plotter functions, and unit tests. perhaps these should go in the methods files? maybe not. think more
6.5) DONE Go back and add joint Box-Cox to cleandat, and unit tests
6.75) DONE get the .Rmds going and compile everything as a package
8) DONE start a vignette, covering the code you have written so far - done except for surrogates, have to do that section, but leaving that until after coherence stuff is written
9) DONE compile again as a package
10) DONE stopping point - mean field methods implemented. next step coherence methods.
11) DONE Work on coherence functionality, inc unit tests and vignette and making the package compile. Need to start with a design phase and working out appropriate S3 classes. See some thoughts below. - IMPLEMENTED COH, STILL NEED PLOTTERS, THENA  VIGNETTE
DONE addranks
DONE cohbandsignif
DONE plotmag
DONE plotrank
DONE plotphase
DONE avgphase
DONE vignette - make sure to cover all these new functions
12) DONE design phase for wavelet linear models
13) to do for wavelet linear models:
DONE wlm and tests  
DONE wlm set and get methods 
DONE wlmtest generator function and accompanying wlmfit internal, and tests 
***have not yet implemented the fast algorithm
***also, the only normalization that is properly implemeneted, throughout the wlm codes, is powall
DONE do you want to add slots for scale.min, scale.max.input, sigma, and f0 to wt, wmf, wpmf, and coh methods? You would also need set and get methods for these slots for each class. Or maybe stick them together into a list called wopts which makes a single slot, and then you would have to make a similar change to wlm.
DONE DAN: check the vignette for what effects the above change would have had, both on runs and on descriptions
DONE wlmtest methods, and tests thereof
DONE plotmag method for wlmtest and tests - docs to be combined with plotmag.coh
DONE plotrank method for wlmtest and tests - docs to be combined with plotrank.coh
DONE tests for addranks for wlmtest objects
DONE bandtest for wlmtest objects, and tests - docs combned with coh method
DONE predsync and tests - wrote the function and some tests, but need to go through some results with Lawrence, see the bottom of tests_predsync.R, and then formalize a few more tests
DONE power function for tts class and tests
***modsync and tests
DONE propexpl and tests - proportion of synchrony explained as a function of timescale and related quantities - ended up calling it syncexpl
***vignette - see about using the plankton data, the version we said we would release if the plankton paper is published. Include in package and base the wlm part of the vignette on these data. Got permission from Chris to do this, but waiting until Lawrence's plankton paper is accepted.
***looscore and tests
***locassoc tester function and tests
***examining if phases of coefficients are consistent with predictor/response phases - do this later, together with functionality for plotting phases of various things (I have left a lot of phase plot functions out)
12) DONE everything you need for deer
***13) synmat for kelp and Jon's bird stuff
DONE The functions to work from in reumannplatz are synmat, SynNetModules, cluster_eigen, and modularity. These are in reumannplatz. Ignore the older functions wsynmat and ModularityDecomp, which were superseded by the above but never removed from reumannplatz.
To do with regard to the clustering stuff:
***spec the methods associated with clust
DONE fill in cluseigen
DONE tests
DONE adapt the modularity code in wsyn
DONE tests 
DONE tests for wavmatwork
DONE fill in synmat - only the fast algorithms remaining
DONE tests - only the fast algorithms remaining
DONE write clust
DONE tests
DONE set and get methods for clust
DONE tests
DONE addwmfs
DONE tests
DONE addwpmfs 
DONE tests

***Need more tests for modularity and cluseigen.
  *cluseigen needs tests comparing directly to the igraph function - added these for unweighted graphs only and many of the tests currently fail badly
  *Lei offered to make a test comparing to by-hand results for the modularity function, let him. Should be for a case with negative entries in the adjacency matrix. Maybe a couple of these. Should test not only the total but also the components.
  *would also like such a test for cluseigen. could do a qualitative one (DAN - do it) but also want a quant one based on by-hand calculations
  *how do we know the eigenvalue method of Newman works for the modularity definition of Gomez? Gomez does not provide a generalization of Newman's eigenvalue method to his new modularity, where did Lei get this? Does this need to be spelled out somewhere? I don't think Gomez even used Newman's eigenvalue method on his new definition.
  *I do not understand the choices made for plotting node strength. The choice was made by Jon, back when we were using the algorithms and definitions of Newman, before we could properly deal with negative entries in the adjancency matrix. It was then copied by Lei once the new approach that could deal with negatives was installed, but I have seen no argument that it still applies, even if it did before. This is more about mathematical thinking than it is about any unit tests. Do we use for node strength the contribution of a node to modularity remapped to (0,1), or rescaled to have max value 1, or something else? Or do we provide options? Jon cites the Fletcher paper as the source for his node strength definition, but I need to understand that, and for one thing I do not think Fletcher is working with adjacency matrices with negatives weights. Fletcher also gives more than one quantity associated with each node. I do not understand how or whether either of the quantities provided by Fletcher maps onto the quantity Jon's code provides.
  
***should the nodeQrs output of modularity be rescale how? or options to the user? does this even make sense as a measure of strength of membership of a module in its node? See reference that Jon sent (Fletcher), also the original clustering algorithm pub, which I believe indicated module membership strength results were available. Get to the bottom of this.
***other methods for clust - plotmap, plotdend, plotmns, plotwmfs, plotwpmfs as methods for the clust class - see below description
***tests
***There are lots of clustering algorithms in igraph (cluster_*) that could be used as options in clust for the clustering if we were willing to make negative weights in the adjacency matrix 0. This is assuming a graph in igraph does not allow negative weights, but that assumption *might* be wrong. Also, we would have to explore the individual algorithms and see if/when they work with negative weights, even if igraph does. igraph seems to allow weights (at least positive ones), but it might not be the standard in that package, so would also have to make sure the clustering algorithms in there are making use of the weights.
DONE To get a speedup, and for other reasons, it may be worth improving the fast coherence algorithm as it exists in synmat as follows:
DONE -write a version of the algorithm (currently in coh) that takes the necessary random numbers as inputs. persumably coh would then call that code after generating the random numbers
***test against Lawrence's Matlab algorithm for exact agreement, now that we can input the random variables
DONE -then synmat would generate one set of random numbers for all pairwise coherence significance calculations it needs to do, and pass the same set to all pairwise calculations of coherence significance
DONE -having the version of the fast coherence algorithm that accepts random numbers as inputs will also be useful for other reasons, e.g., comparing to matlab code results
DONE Lawrence says it should be possible to test the significance of the real part of the cross wavelet transform using an adapted version of his fast algorithm
***14) write the phase plotter functions, and unit tests, for tts, wt, wpmd, wmf. 
***15) print and summary methods for all the classes


for formatting the documentation at top:
1) use \code{} for TRUE and FALSE. Do not use T and F.
2) I foudn and deleted a comment in the header of wmpf to the effect that "the wavelet phasor mean field was developed by Sheppard and Reuman, R code written by Anderson and Walter." It was not developed by Shppard and Reuman, and now the R code is by all of us, so I changed it. Watch out for other notes like that, I think I remember seeing some more.
3) homogenize the emails that are indicated for each author, and make them be current/permanent emails. Even better, store the emails centrally and have all docs import them somehow.

general
1) DECIDED NOT TO BOTHER It will be possible to be a bit more consistent with use of get and set methods instead of $
2) DONE you need an error checking function for scale.min, scale.max, f0, sigma, and you need to call it from the top of all functions that take these as arguments. Also
indicate in vignette and in help files what the constraints are. 
3) DONE times can ONLY be spaced with units of one, otherwise the code does not
work as it is. This is a fundamental problem and would require changes in lots of 
places if we wanted to add the flexibility of times spaced other than 1. So just add this requirement to your error checking. Also in docs and vignette. Everything is in 
cycles per sample - Lawrence assumes this in his matlab code. 
4) do a test case with Lawrence for non-integer scale.min. For that matter do some
testing with non dedault values of all of scale.min, scale.max, f0, sigma. One thing to test is, if you use scale.max.input very large, then scallopping should produce
the actual largest timescale and this should be the same for even larger scale.max.input. I said this was the case in the vignette (toward the end of the wt section), so if it turns out not to be the case, go back and change that part of the vignette
5) DONE Add a paragraph to the vignette explaining what scale.min, scale.max, f0, sigma,
or ask Lawrence to do it. 
6) DONE test coh with vectors, also with 1 by N matrices
7) right now, when using the fast method, coh does not work with norm=powind
8) DONE error check the fast algorithm, n=1 case
9) Add examples to example sections of docs everywhere

wt.r:
#TODO: make this a compiled function using cmpfun in the compiler package
#(which is part of base R)
#TODO: make the truncation occur before the computation rather than after?
#TODO: Not clear from the docs what the resulting behavior is for the default
#value NULL of scale.max.input. Likewise for other related funcs (wmf, wpmf, etc)

________

Design phase for wavelet linear models:

Functionality needed:
*Fit a given model - pass in data, and a normalization to use (or should we only use the power normalization), and it does the fitting and gets coefs that depend on timescale (only, not on loction). It will also need scale.min, f0, etc.
*May want the capacity to plot the coefficients in various ways, e.g., plot their phases or magnitudes against timescale, but if you do this, do it much later
*Compare two models. Will need to be able to plot coherences and ranks, and get p-values over a timescale band. Will have to work for fft or aaft surrogates, or via the fast algorithm. May or may not want to have the option of joint surrogates of multiple variables being dropped versus independent (across the variables) surrogates. Not sure if the latter makes any sense - discuss with Lawrence. Of course, spatially, we always use joint surrogates.
*model-predicted synchrony plot
*think a bit to make sure the design would permit reasonable addition, later, of functionality for doing the leave-one-out cross validation and model selection, but don't do it now.
*proportion of synchrony explained by a model, as a function of timescale (and this can be averaged over a timescale band). See eq 47 in the supp mat of the plankton paper.
*cross terms as a proportion of synchrony, as a function of timescale (and this can be averaged over a band). See eq. 48 of supp mat of plankton paper
*do you want to be able to plot model synchrony or mean squared model synchrony? (remember this is distinct from model-predicted synchrony.) We used this for thinking of interaction effects.
*do anything else with interaction effects, e.g., test their significance?
*fractions of synchrony explained by individual predictors, and by interaction effects, as a function of timescale. Also can average these over bands.

Thoughts:
have a wlm (wavelet linear model) class with these slots:
dat: a list with the data matrices, probably the first is the response
times: the times of measurement
wts: a list with all the wavelet transforms (do you actually want this? I don't see why not except if we end up creating one of these objects for each of many surrogates, we won't want to keep all the transforms). Perhaps it should be powall normalized transforms?
timescales: 
coefs: a list with coefficients - these are complex-valued functions of timescale, one for each predictor
d2/resids: the measurement of how good the model is (also a function of timescale, I think)
coher: coherence of model with response (do you actually want this? or perhaps we want it even more than d2/resids?)

You need a fast, internal function that fits pre-computed transforms and just returns the coefficients and the residuals. Call this (for now) wlm_fit_intern. Or maybe it also returns coherence of the fitted model with the response. Or maybe only.

You need a fitter function (user side) that takes data (and other arguments) and produces a wlm object - this is the creator for a wlm object

You need a wlm_test (temporary name) function that takes a wlm object and other arguments and performs significance tests for one or more variables as specified in the other arguments. This will have several arguments specifying the nature of the test (fft surrog, aaft surrog, fast, possibly also joint v independent surrogates of the separate surrogated variables). For fft surrog or aaft surrog, this generates surrogates of the variables to be tested, recomputes transforms, then calls wlm_fit_intern, then keeps all the residual vectors (and maybe also or instead coherences of the models with the data) and throws away the coefficients, then returns this stuff somehow. 
  Returns another type of object, or embedded in the wlm object? If the latter, then it will be easier to loose track of which test objects go with which model object, but if the latter the model object can become very complex. perhaps we want a wlm_test class whose first slot is a wlm object, indicating the model being tested. Next slots can store the variables that are removed for the test, and the algorithm used (fft, aaft, fast). Next slots after that can store the residuals for each surrogating, and in addition or instead model/response coherences for each surrogating.
  For the fast algorithm things will prbably be different, and will probably start from the data in the wlm object instead of from the transforms

Lawrence tests based on residuals d, but I would prefer to plot and test based on coherences. Mathematically it is equivalent but I would prefer to see plots that show coherence versus timescale for the real model and then for versions where some predictors have been replaced with surrogates. The problem with this is, defining coherence requires that one choose a normalization, and we only know that d minimization and coherence maximization are related for powall normalization. So at a minimum one needs to think carefully and do some math about how normalization and fitting and coherence interact. But I think actually powall is baked into the pie for this stuff, and I just will be committing to it, but check this with Lawrence.

I have some code here from before that is probably pretty good and useful: Reuman/ResearchStaff/PeopleInAction/LawrenceEmmaSVN_2015/CodePackages/WaveletCode/Waveletanalysis/

Everything has to work equally for single or multiple time series per variable.

What about normalization options? When do those come in? I think they probably are not involved with the actual fitting (NOT TRUE THEY ARE), they only come in at that stage of working out fractions of synchrony explained and so on? Or is the powall normalization baked into the pie? It certainly is baked in to the math in Appendix S7 in the plankton paper.

Qs for Lawrence
1) joint vs. independent surrogates of separate predictor variables when testing multiple predictor variables? Or does only joint make sense? Does the fast algorithm support simultaneous removal of more than one variable? I assume in that case it'll be joint surrogates that are implemented? Could independent surrogs be implemented, if they make any sense?
Lawrence and Dan decided to only implement the joint surrogates  - cannot find a case where it would make sense to do the indepednent surrgates
2) Inputs and outputs of fast algorithm? Does it start from the data, or the transforms? What does it return?
Args: data matrices
Outputs: coherence between model and response variable, and/or squared norm of d, the residuals. Actually, these things do not count as outputs because Lawrence does not normalize in the same way, he uses these items (which are not the same as what he would get if he normalized) to get ranks (which are the same as what he would get). So actually the rank information is what is outputted, specifically modelpow and surrmodelpow, where: modelpow is power of response minus norm of d^2. Lawrence reckons he could fairly trivially post-hoc implement the right normalization. Lawrence says I can assume his algorithm will take daata and specs on which vars are dropped and spit out coher (powall norm) for model/data for real and surrogates.
3) Is powall normalization baked into the pie? 
See above - make it so you can put in powall, powind, or none, and make it throw a not yet implemented error for powind - Lawrence thinks he can do this later. Currently it does none, it can do powall with a rescaling, and powind will take work. For the slow algorithm, there may be some thought required for both none and definitely for powind, so maybe just implement powall right now and throw and error for the others.

_____

Next step in design:

DONE wlm class:
slots:
dat - a list of matrices, all the same dimensions, representing the data, the first is the response
times - times of measurement
norm - the normalization used 
wts - list of transforms, normalized as specified in argument to generator function, but only powall works now, other options throw an error
timescales - 
coefs - list of complex vectors, each of length the same as timescales. these are the model coefs
coher - appropriately normalized version of coherence of the  model and the response transforms, as specified in norm argument to geneator function, but only powall works for now

DONE wlm generator function:
Args:
dat - a list of matrices representing the data (or in the case of 1 location, a list of vectors)
times - times of measurement
resp - an index in dat of the response matrix
pred - a vector of indices in dat of predictor matrices
norm - powall, none, powind (phase not allowed). Everything but powall throws and error for now
Returns:
A wlm object with the fitted model
What it does:
-rearrange dat to put the response first and keep only the used predictor matrices after that; use vectors when the input data were vectors 
-error check for things like all data matrices/vectors have same dimensions
-compute all transforms to get wts, which should always be locations by times by timescales, even when there is only one location
-pass wts to a fast internal fitting function to get the other slot values
-put it all together into a list and set the class attribute

DONE wlm_fit (an internal function, no error checking):
Args: 
wts - the list of normalized transforms, response variable first
norm - possibly need the normalization used, either powall, none, or powind; everything but powall throws and error for now
Resturns: A list with these elements:
coefs - list of complex vectors, each of length the same as timescales. these are the model coefs
coher - coherence of the model and response transforms, normalization appropriate for the norm argument (function of timescale, real valued)

DONE wlmtest class:
slots:
wlm - a wlm object, the model being tested
drop - either names or indices of variables in wlm$dat that are being dropped. Some error checking is needed to make sure these are legit, e.g., not the response variable
signif - list with coher (appropriately normalized coherence) and scoher (appropriate normalized coherence for surrogates), signif$coher the same as wlm$coher, except for "fast" where these can differ
ranks - similar to the ranks slot for the coh class. As in that case, not filled in on the first call to wlmtest. I rewrote addranks so it will work on either a coh or a wlmtest object. 
bandp - similar to coh class. cohbandtest (now bandtest) is already a method for the coh class, so I will just need to add a method for the wlmtest class. To avoid repeating myself I may want to take guts of the routine and put it into a worker function called by both methods. 

DONE wlmtest generator function:
Args:
wlm -  a wlm object
drop - indictaes which variables are dropped, see arg with same name for wlmtest class
sigmethod - either fft, aaft, fast
Returns: 
A wlmtest object
Notes:
The only combinations of wlm$norm and sigmethod that will work at first will be powall with anything

DONE plotmag and plotrank methods for wlmtest objects should be pretty straightforward, as well as you will need something like addranks and cohbandtest - see thoughts above in ranks and bandp slots of the wlmtest object, some of this is already done (addranks), and some of it has been queued up (bandtest)

You may want to have a plotcoef method for the wlm class, but this should be thought about together with plotting the phase and maybe magnitude of the response times the conjugate of the predictor, as we did in the plankton paper to help validate the models there - do this later

_____


Think about whether this design will work with the following outstanding issues:

*model-predicted synchrony plot

DONE >>>make a function predsync that takes a wlm objetc and returns a tts object which is what we give on line 413 of the supp mat of the plankton paper, but without the magnitudes (and recall the Pi there is already a nonnegative real number for each timescale, before taking magnitudes). Then that can be plotted easily with the plotmag method of the tts class, or its plot v timescale of the mean over time of the squared magnitude. 
DONE I could write a method for a tts object that plots, vs timescale, the mean, over time, of the square magnitude. That would be an easy function. Should I call it power? Remind yourself what power is officially defined as, but regardless this would be easy. Yes, I checked with Lawrence, this is power.
Then I could easily plot model-predicted synchrony

*do you want to be able to plot model synchrony or mean squared model synchrony? (remember this is distinct from model-predicted synchrony.) We used this for thinking of interaction effects.

>>>make a function modsync that takes a wlm object and returns a tts object which is what we denote r_\sigma^{(h)}(t) in the plankton paper. This is a complex-valued function of time and timescale. We can then plot its magnitude using the plotmag method of the tts class. If you want to average the square of this magnitude over time, and then plot that (which is what we do in the plankton paper), it will be easy to do so with your own code. Or I could write a method for a tts object that plots, vs timescale, the mean, over time, of the square magnitude. That would be an easy function. Should I call it power? Remind yourself what power is officially defined as, but regardless this would be easy. Yes, I checked with Lawrence, this is power.

*think a bit to make sure the design would permit reasonable addition, later, of functionality for doing the leave-one-out cross validation and model selection, but don't do it now. 

>>>I think this should be possible after the fact. You'll just write a function that computes a leave-one-out score, and that will make it possible for the user to get the score and compare for different models. The biggest thing to figure out is, the norm squared residuals are used in the description on p.8 of the sup mat of the plankton paper, but I was hoping to not have to ever compute residuals, just use the coherence of model with data. But I think that can be resolved later.  

*Maybe also think about the "local ascociations" test you did. 

>>>Probably the local associations stuff can be left out - would not be too hard with existing code. But if you do later want a local associations test, it would be easy to write a separate function that does that. It would just take two data matrices, just like coh does. Just re-read the section on location permutations in the plankton paper supp mat and you will see this would be a straightforward later addition. In fact I decided this should be done at some point, it won't be hard.

*Some other blocks of text I have elsewhere have started thinking about testing phases of coefficients

>>>Can safely leave this for later, I think

*proportion of synchrony explained by a model, as a function of timescale (and this can be averaged over a timescale band). See eq 47 in the supp mat of the plankton paper.
*cross terms as a proportion of synchrony, as a function of timescale (and this can be averaged over a band). See eq. 48 of supp mat of plankton paper
*fractions of synchrony explained by individual predictors, and by interaction effects, as a function of timescale. Also can average these over bands.

DONE >>>Make one function that takes a wlm object and gives a matrix with named rows:
timescales
propexplained
crossterms
pred1
...
predn
interactions
int1w2
int1w3
...
intnwn-1
Then these can easily be plotted versus timescale or averaged over any band.
Though maybe the above should be columns of a data frame instead

*do anything else with interaction effects, e.g., test their significance?

>>>Spoke with Lawrence and we think this could be handled pretty easy with the high-level functions already planned above, and no additional function is needed.

____

Design for clustering functionality:

***Lei's function cluster_eigen in reumannplatz:
*Args: an adjacency matrix
*Provides that I can use: membership list (without weights of membership) for each node at each split stage. It can also be made to plot a dendrogram, but this part seems half-baked/faulty (he admits it in the docs).
*I should create a function that provides the membership list for each node at each split stage, but does not do the dendrogram. Should probably be a fast internal function.
*Qs for later: will I want the modularity at each step? remind yourself what does that mean anyway? I just learned the cluster_eigen does not compute modularity anyway, it only calls out to the modularity function for it, and it only does that for the purpose of plotting! So I think it is OK to not involve modularity at all in what is returned by this function.
*If I use the dendrogram plotting stuff, it should be in some plotting method of some class that stores the output of all this stuff that I am working on - should be done after the fact.
*so I can start with Lei's function, move it into wsyn and edit

***so the new function spec should be:
*Args: an adjacency matrix, same error checking as currently
*Output: a list of the membership vectors for each split, the last element being the final one
*no side effects
*internal and as fast as possible. actually, decided I might as well export it.

***modularity function in reumannplatz:
*ANSWERED Qs: Do we need the membership argument if decomp if F? It provides a 0 in an example I tried with membership NA and decomp F, which may suggest a bug? The call to modularity in cluster_eigen *does* provide the membership argument (and uses the default of F for the decomp arg), but one wonders why it is needed - should it not be possible to compute modularity without having done the clustering? After reading the help files for igraph::modularity, which Lei's function is supposed to replace, I have some answers to this q. This is not modularity of a network, but modularity of a division of a graph into subgraphs. The stat itself is described in a formula in the help file to igraph::modularity. 
*ANSWERED Qs: do we really need the decomp stuff, or is a simplified function better for our purposes? I discovered that modularity is called by SynNetModules with decomp=T, and the results are returned and also used in the plotting routine, apparently for strength of membership stuff. So I think the full functionality is needed.
*Nice unit testing fact: the values of modularity and igraph::modularity are supposed to agree when graph weights are positive.
*Args: probably should stay exactly the same
*Outputs: probably should stay exactly the same
*So this one just need to be cleaned up and unit tested, pretty much.
*not sure whether to make it internal or external, but leaning toward internal

***so the new function spec should be:
*Args: probably should stay exactly the same
*Outputs: probably should stay exactly the same (though possibly rename them)
*leaning toward making it internal. Actually, I decided I might as well export it.

***Lei and Jon's synmat:
*Args: diverse, and which are used depends on the input "method"
*Output: the synchrony matrix
*I think the inputs and outputs of this are bascally right but it needs to be adapted to wsyn and cleaned up. 
*One way to clean up the input has to do with the fact that many input args are only used for some methods choices. So we could have an input "methodspecs" with is a named list including all the specs. There would be a complex error checking function for these things. The first entry would be "method", which would be a string of one of the allowed valued (listed in docs) indicating the method. The rest of the list would have allowed values which would depend on that string (there are different specs for pearson v ReXwt). Then again, maybe it is better the way it is, because with the sensible defaults that are selected, the user can avoid thinking about irrelevant options.
*probably you want to move the weighted and alpha arguments from SynNetModules so they are here, too

***Jon and Lei's SnNetModules:
*Currently it does too much - all the plotting is included. Should seperate the plotting and the calculations by using an S3 class.
*My thought about a "methodspecs" argument decribed above probably don't apply to synmat, but may well apply to SynNetModules, since additional arguments are needed here, and all this stuff should anyway be saved in an S3 object created. Alternatively, you could have all the arguments separate for the function call but put them all in a methodspecs slot in the object created. That seems best.
*rename the function - it will be the generator function for a class
*What should be in the S3 class?
The inputs: data, times, coords (maybe other metadata), methodspecs
The adjacency matrix that defines the network
The list of memberships at each split
Probably the complete results of a call to the modularity function at each split
mns - mean time series for modules, starts out NA only added with a call to addmeans, a method for the class. mns will have to have the means for each level of splitting. Or perhaps it only has the ones that have been requested so far?
wmfs - wmf objects for modules. Starts out NA, only added with a call to addwmfs, a method for the class. wmfs will have to be a list (for different levels of split) of lists (for different modules within a split) of wmf objects. Only the ones requested so far are filled in.
wpmfs - similar to wmfs, but for phasor mean fields
*this design basiaclly seems to make sense, but it is very complex. my hope is, to the user it would not seem complex because most of the complexity is handled internally.
*methods and functions that go along with the class:
the generator (exported function) 
addmns, addwmfs, addwpmfs (exported methods)
plot (exported method, takes an argument for the split wanted, with default the final split; also takes arguments for what is to be displayed (mns, wmfs, wpmfs, etc), and cross references those with what is available in the object (has been add with addmns, addwmfs, etc) and plots what is both requested and avaialble with a warning when something requested was not available, or maybe it just fills it in and returns the new object? or maybe it just throws and error if it has not been added?) Also maybe plots the dendrogram? Or is that a separate function? Or the same function but a seperate plot?
*After more thought I decided such a plot function would be a huge pain to write. For one thing, we cannot control how many modules there will be, and plotting large numbers of modules on one complex multipanel divice is a pain. It's a pain anyway. My thinking has been so far to leave complex preduction-quality plotting to end users. So Let's do it instead as follows:
*addmns, addwmfs, addwpmfs are all internals. We have methods for the clust class: plotmap, plotdend, plotmns, plotwmfs, plotwpmfs, all of which take a clust object and a level of split argument, and make the associated plots, on *multiple* panels when needed (e.g., plotwmfs will use separate panels or pdfs for each cluster).







